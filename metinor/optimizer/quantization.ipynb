{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantizing a PyTorch Model\n",
    "\n",
    "Quantization is a process that reduces the precision of the weights and activations of a neural network, converting them from floating point to lower precision formats like integer or fixed point. This can significantly reduce the model size and increase inference speed, while aiming to maintain the original model's accuracy.\n",
    "\n",
    "The quantization package in Metinor offers a straightforward API to apply various quantization strategies to your neural network models. In this tutorial, we will use Metinor to qauntize a PyTorch model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supported Quantization Strategies\n",
    "\n",
    "First, to see which quantization strategies are available, use the `list_quantization_strategies` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WeightOnlyQuantization', 'WeightBiasQuantization']\n"
     ]
    }
   ],
   "source": [
    "from metinor.optimizer import list_quantization_strategies\n",
    "\n",
    "strategies = list_quantization_strategies()\n",
    "print(strategies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Quantization API\n",
    "\n",
    "The `quantize` function in Metinor's quantization package is the main entry point for quantizing a model. It takes a PyTorch model, a quantization strategy, and a dictionary of configuration options as input, and returns a quantized model.\n",
    "\n",
    "### Create a Model\n",
    "\n",
    "First, let's create a simple neural network model using PyTorch. For demonstration purposes, we will use the [LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) architecture from LeCun et al., 1998."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square conv kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5x5 image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = LeNet().to(device=device)\n",
    "input_shape = (1, 1, 32, 32)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Functions\n",
    "\n",
    "Next, we import the required functions from the `metinor.optimizer.quantization` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import quantization functions\n",
    "from metinor.functional.quantization import (\n",
    "    quantize,\n",
    "    quantize_node,\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quantize a model, you will use the `quantize` function. This function requires specifying the model, the quantization strategy, the type of quantization, and the precision level.\n",
    "\n",
    "### Weight-Only Quantization\n",
    "\n",
    "This strategy quantizes only the weights of the model to the specified precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): QuantConv2d(\n",
      "    1, 6, kernel_size=(5, 5), stride=(1, 1)\n",
      "    (input_quant): ActQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "    (output_quant): ActQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "    (weight_quant): WeightQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "      (tensor_quant): RescalingIntQuant(\n",
      "        (int_quant): IntQuant(\n",
      "          (float_to_int_impl): RoundSte()\n",
      "          (tensor_clamp_impl): TensorClampSte()\n",
      "          (delay_wrapper): DelayWrapper(\n",
      "            (delay_impl): _NoDelay()\n",
      "          )\n",
      "        )\n",
      "        (scaling_impl): StatsFromParameterScaling(\n",
      "          (parameter_list_stats): _ParameterListStats(\n",
      "            (first_tracked_param): _ViewParameterWrapper(\n",
      "              (view_shape_impl): OverTensorView()\n",
      "            )\n",
      "            (stats): _Stats(\n",
      "              (stats_impl): AbsMax()\n",
      "            )\n",
      "          )\n",
      "          (stats_scaling_impl): _StatsScaling(\n",
      "            (affine_rescaling): Identity()\n",
      "            (restrict_clamp_scaling): _RestrictClampValue(\n",
      "              (clamp_min_ste): ScalarClampMinSte()\n",
      "              (restrict_value_impl): FloatRestrictValue()\n",
      "            )\n",
      "            (restrict_scaling_pre): Identity()\n",
      "          )\n",
      "        )\n",
      "        (int_scaling_impl): IntScaling()\n",
      "        (zero_point_impl): ZeroZeroPoint(\n",
      "          (zero_point): StatelessBuffer()\n",
      "        )\n",
      "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
      "          (bit_width): StatelessBuffer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (bias_quant): BiasQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "  )\n",
      "  (conv2): QuantConv2d(\n",
      "    6, 16, kernel_size=(5, 5), stride=(1, 1)\n",
      "    (input_quant): ActQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "    (output_quant): ActQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "    (weight_quant): WeightQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "      (tensor_quant): RescalingIntQuant(\n",
      "        (int_quant): IntQuant(\n",
      "          (float_to_int_impl): RoundSte()\n",
      "          (tensor_clamp_impl): TensorClampSte()\n",
      "          (delay_wrapper): DelayWrapper(\n",
      "            (delay_impl): _NoDelay()\n",
      "          )\n",
      "        )\n",
      "        (scaling_impl): StatsFromParameterScaling(\n",
      "          (parameter_list_stats): _ParameterListStats(\n",
      "            (first_tracked_param): _ViewParameterWrapper(\n",
      "              (view_shape_impl): OverTensorView()\n",
      "            )\n",
      "            (stats): _Stats(\n",
      "              (stats_impl): AbsMax()\n",
      "            )\n",
      "          )\n",
      "          (stats_scaling_impl): _StatsScaling(\n",
      "            (affine_rescaling): Identity()\n",
      "            (restrict_clamp_scaling): _RestrictClampValue(\n",
      "              (clamp_min_ste): ScalarClampMinSte()\n",
      "              (restrict_value_impl): FloatRestrictValue()\n",
      "            )\n",
      "            (restrict_scaling_pre): Identity()\n",
      "          )\n",
      "        )\n",
      "        (int_scaling_impl): IntScaling()\n",
      "        (zero_point_impl): ZeroZeroPoint(\n",
      "          (zero_point): StatelessBuffer()\n",
      "        )\n",
      "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
      "          (bit_width): StatelessBuffer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (bias_quant): BiasQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_quant_weightonly_float16 = quantize(model, \"WeightOnlyQuantization\", \"float\", 16)\n",
    "print(model_quant_weightonly_float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compare the printed model summary before and after quantization, you will see that the `Conv2d` layers have been replaced with `QuantizedConv2d` layers with a `WeightQuantProxyFromInjector` module. This indicates that the weights of the model have been quantized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight-Bias Quantization\n",
    "\n",
    "This strategy quantizes both the weights and biases of the model to the specified precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_quant_weightbias_int8 = quantize(model, \"WeightBiasQuantization\", \"fixed\", 8)\n",
    "print(model_quant_weightbias_int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layerwise Quantization\n",
    "\n",
    "Instead of quantizing the entire model at once, you can also quantize specific layers of the model. This can be useful if you want to apply different quantization strategies to different parts of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Module:  Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "Updated Node:  QuantConv2d(\n",
      "  1, 6, kernel_size=(5, 5), stride=(1, 1)\n",
      "  (input_quant): ActQuantProxyFromInjector(\n",
      "    (_zero_hw_sentinel): StatelessBuffer()\n",
      "  )\n",
      "  (output_quant): ActQuantProxyFromInjector(\n",
      "    (_zero_hw_sentinel): StatelessBuffer()\n",
      "  )\n",
      "  (weight_quant): WeightQuantProxyFromInjector(\n",
      "    (_zero_hw_sentinel): StatelessBuffer()\n",
      "    (tensor_quant): RescalingIntQuant(\n",
      "      (int_quant): IntQuant(\n",
      "        (float_to_int_impl): RoundSte()\n",
      "        (tensor_clamp_impl): TensorClampSte()\n",
      "        (delay_wrapper): DelayWrapper(\n",
      "          (delay_impl): _NoDelay()\n",
      "        )\n",
      "      )\n",
      "      (scaling_impl): StatsFromParameterScaling(\n",
      "        (parameter_list_stats): _ParameterListStats(\n",
      "          (first_tracked_param): _ViewParameterWrapper(\n",
      "            (view_shape_impl): OverTensorView()\n",
      "          )\n",
      "          (stats): _Stats(\n",
      "            (stats_impl): AbsMax()\n",
      "          )\n",
      "        )\n",
      "        (stats_scaling_impl): _StatsScaling(\n",
      "          (affine_rescaling): Identity()\n",
      "          (restrict_clamp_scaling): _RestrictClampValue(\n",
      "            (clamp_min_ste): ScalarClampMinSte()\n",
      "            (restrict_value_impl): FloatRestrictValue()\n",
      "          )\n",
      "          (restrict_scaling_pre): Identity()\n",
      "        )\n",
      "      )\n",
      "      (int_scaling_impl): IntScaling()\n",
      "      (zero_point_impl): ZeroZeroPoint(\n",
      "        (zero_point): StatelessBuffer()\n",
      "      )\n",
      "      (msb_clamp_bit_width_impl): BitWidthConst(\n",
      "        (bit_width): StatelessBuffer()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (bias_quant): BiasQuantProxyFromInjector(\n",
      "    (_zero_hw_sentinel): StatelessBuffer()\n",
      "    (tensor_quant): RescalingIntQuant(\n",
      "      (int_quant): IntQuant(\n",
      "        (float_to_int_impl): RoundSte()\n",
      "        (tensor_clamp_impl): TensorClampSte()\n",
      "        (delay_wrapper): DelayWrapper(\n",
      "          (delay_impl): _NoDelay()\n",
      "        )\n",
      "      )\n",
      "      (scaling_impl): StatsFromParameterScaling(\n",
      "        (parameter_list_stats): _ParameterListStats(\n",
      "          (first_tracked_param): _ViewParameterWrapper(\n",
      "            (view_shape_impl): OverTensorView()\n",
      "          )\n",
      "          (stats): _Stats(\n",
      "            (stats_impl): AbsMax()\n",
      "          )\n",
      "        )\n",
      "        (stats_scaling_impl): _StatsScaling(\n",
      "          (affine_rescaling): Identity()\n",
      "          (restrict_clamp_scaling): _RestrictClampValue(\n",
      "            (clamp_min_ste): ScalarClampMinSte()\n",
      "            (restrict_value_impl): PowerOfTwoRestrictValue(\n",
      "              (float_to_int_impl): CeilSte()\n",
      "              (power_of_two): PowerOfTwo()\n",
      "            )\n",
      "          )\n",
      "          (restrict_scaling_pre): LogTwo()\n",
      "        )\n",
      "      )\n",
      "      (int_scaling_impl): PowerOfTwoIntScaling()\n",
      "      (zero_point_impl): ZeroZeroPoint(\n",
      "        (zero_point): StatelessBuffer()\n",
      "      )\n",
      "      (msb_clamp_bit_width_impl): BitWidthConst(\n",
      "        (bit_width): StatelessBuffer()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from metinor.visualizer import get_graph\n",
    "from metinor.functional.quantization import quantize_node\n",
    "\n",
    "# Create model graph with maximum depth\n",
    "graph = get_graph(model, input_size=input_shape, depth='max', device=\"cpu\")\n",
    "\n",
    "# Create a dictionary of node_id: module\n",
    "node_ids = list(graph.id_dict.keys())\n",
    "node_module_dict = {}\n",
    "for node_id in node_ids:\n",
    "    node = graph.find_node_by_id(node_id)\n",
    "    node_module_dict[node_id] = node\n",
    "\n",
    "# Quantize layer by id\n",
    "node = node_module_dict[node_ids[1]]\n",
    "print(\"Node Module: \", node.module_unit)\n",
    "\n",
    "quantized_module = quantize_node(\n",
    "    node_ids[1], graph, \"WeightOnlyQuantization\", \"float\", 4\n",
    ")\n",
    "# print('Quantized Module: ', quantized_module)\n",
    "\n",
    "# Find node again to verify the change\n",
    "node = graph.find_node_by_id(node_ids[1])\n",
    "print(\"Updated Node: \", node.module_unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Quantization Strategies\n",
    "\n",
    "In addition to the strategies demonstrated above, Metinor also supports the following quantization strategies:\n",
    "\n",
    "- **ActivationOnlyQuantization**: Applies quantization only to the activations during inference.\n",
    "- **WeightBiasActivationQuantization**: Quantizes weights, biases, and activations of the model, offering a comprehensive quantization approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
